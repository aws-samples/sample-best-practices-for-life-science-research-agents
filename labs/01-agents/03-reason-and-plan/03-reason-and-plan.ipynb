{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d534971-cb20-4752-9331-a3f98339eced",
   "metadata": {},
   "source": [
    "# Research Planning Agent\n",
    "\n",
    "![Agent with scientific knowledge tool](../../../static/agent-deep.png)\n",
    "\n",
    "This lab teaches participants how to build a sophisticated research planning agent that can decompose complex research questions into manageable tasks and execute them iteratively. The lab represents a significant advancement from the previous labs by introducing autonomous research planning and execution capabilities.\n",
    "\n",
    "The lab begins by demonstrating the difference between agents with and without system prompts, showing how proper guidance dramatically improves response quality and consistency for biomedical research questions.\n",
    "\n",
    "Participants learn to craft detailed system prompts that establish the agent as an expert research lead with specific behavioral guidelines:\n",
    "\n",
    "- Professional tone prioritizing clarity\n",
    "- Precise technical language\n",
    "- Transparent AI identity\n",
    "- Trust-building through consistency, benevolence, and competence\n",
    "\n",
    "The lab introduces Claude 4's beta interleaved thinking feature, which enables the model to reason between tool calls and make adaptive decisions based on intermediate results. This creates a more dynamic research process where the agent can adjust its approach based on what it discovers. Unlike simple question-answering systems, this agent follows a rigorous research process that mirrors academic research practices, including outline development, iterative investigation, and comprehensive synthesis.\n",
    "\n",
    "The combination of planning capabilities, advanced reasoning, and integration with scientific databases creates a powerful foundation for the multi-agent orchestration covered in the final lab.\n",
    "\n",
    "## 1. Prerequisites\n",
    "\n",
    "- Python 3.12 or later\n",
    "- AWS account configured with appropriate permissions\n",
    "- Access to the Anthropic Claude 3.7 Sonnet model in Amazon Bedrock\n",
    "- Basic understanding of Python programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722465ef-f70b-48fc-9b78-d4d8a49f30b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -U boto3 strands-agents strands-agents-tools defusedxml httpx bedrock_agentcore_starter_toolkit paper-qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a000be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic\")\n",
    "warnings.filterwarnings(\"ignore\", module=\"litellm\")\n",
    "MODEL_ID = \"global.anthropic.claude-sonnet-4-20250514-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b8bf78",
   "metadata": {},
   "source": [
    "## 2. Define System Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2543b6",
   "metadata": {},
   "source": [
    "In this section, we'll create a research agent with the ability to break down a complex research question into smaller topics.\n",
    "\n",
    "There are many different ways to approach this. Here are some examples:\n",
    "\n",
    "- [Anthropic Research Lead Agent](https://github.com/anthropics/claude-cookbooks/blob/main/patterns/agents/prompts/research_lead_agent.md)\n",
    "- [HuggingFace Open Deep Research](https://github.com/huggingface/smolagents/blob/main/src/smolagents/prompts/code_agent.yaml)\n",
    "\n",
    "Let's start by defining the agent role and personality. This is important for ensuring our final research report has the level of clarity we need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21740be",
   "metadata": {},
   "source": [
    "### 2.1. No System Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cfd49c",
   "metadata": {},
   "source": [
    "For comparison, here is an example agent response without a system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60669318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "\n",
    "agent = Agent(model=MODEL_ID)\n",
    "response = agent(\"How safe and effective are GLP-1 drugs for long term use?\")\n",
    "\n",
    "response.metrics.accumulated_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680077cf",
   "metadata": {},
   "source": [
    "### 2.2 Define agent identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d805a4",
   "metadata": {},
   "source": [
    "First, we'll give our agent some guidance about it's role and syle. This is important for ensuring the right language level and user safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39eab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "The current date is {date.today().strftime('%B %d, %Y')}\n",
    "\n",
    "You are an expert research lead that answers biomedical questions using scientific literature and other authoritative sources. \n",
    "You maintain user trust by being consistent (dependable or reliable), benevolent (demonstrating good intent, connectedness, and care), transparent (truthful, humble, believable, and open), and competent (capable of answering questions with knowledge and authority).\n",
    "When responding to the user, use a professional tone that prioritizes clarity, without being overly formal.\n",
    "Use precise language to describe technical concepts. For example, use, \"femur\" instead of \"leg bone\" and \"cytotoxic T lymphocyte\" instead of \"killer T cell\".\n",
    "Make your identity as an AI system clear. Don't pretend to be human or include excessive personality, adjectives, or emotional language.\n",
    "\"\"\"\n",
    "\n",
    "print(f'\\nThe current system prompt is:\\n\"\"\"{system_prompt}\"\"\"\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed6c47",
   "metadata": {},
   "source": [
    "Try the same query and observe the difference in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a88aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(model=MODEL_ID, system_prompt=system_prompt)\n",
    "response = agent(\"How safe and effective are GLP-1 drugs for long term use?\")\n",
    "\n",
    "response.metrics.accumulated_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f71f73c",
   "metadata": {},
   "source": [
    "### 2.3. Define the research process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e004beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt += \"\"\"\n",
    "<research_process>\n",
    "Your goal is to help the user by decomposing questions into sub-topics, generating excellent research plans, using specialized tools to retrieve accurate information, and writing comprehensive, accurate research reports.\n",
    "Follow this process to break down the user’s question and develop an excellent research plan. \n",
    "Think about the user's task thoroughly and in great detail to understand it well and determine what to do next. \n",
    "Analyze each aspect of the user's question and identify the most important aspects. \n",
    "Consider multiple approaches with complete, thorough reasoning. \n",
    "Explore several different methods of answering the question (at least 3) and then choose the best method you find. \n",
    "\n",
    "Follow this process closely:\n",
    "\n",
    "1. **Assess the question**: Analyze and break down the user's prompt to make sure you fully understand it.\n",
    "\n",
    "  - Identify the main concepts, key entities, and relationships in the task.\n",
    "  - List specific facts or data points needed to answer the question well.\n",
    "  - Note any temporal or contextual constraints on the question.\n",
    "  - Analyze what features of the prompt are most important - what does the user likely care about most here? What are they expecting or desiring in the final result? What tools do they expect to be used and how do we know?\n",
    "  - Determine what form the answer would need to be in to fully accomplish the user's task. Would it need to be a detailed report, a list of entities, an analysis of different perspectives, a visual report, or something else? What components will it need to have?\n",
    "\n",
    "2. **Determine the question type**: Explicitly state your reasoning on what type of question this is from the categories below.\n",
    "\n",
    "  - **Straightforward question**: When the problem is focused, well-defined, and can be effectively answered by a single focused investigation or fetching a single resource from the internet.\n",
    "    - Can be handled effectively by your innate knowledge or a single tool; does not benefit much from extensive research.\n",
    "    - Example 1: \"Tell me about bananas\" (a basic, short question that you can answer from your innate knowledge)\n",
    "    - Example 2: \"Who developed the ESM3 protein model?\" (simple fact-finding that can be accomplished with a simple literature search)\n",
    "\n",
    "  - **Deep research question**: When the problem requires multiple perspectives on the same issue or can be broken into independent sub-questions.\n",
    "    - Benefits from parallel research efforts exploring different viewpoints, sources, or sub-topics\n",
    "    - Example 1: \"What are the most effective treatments for depression?\" (benefits from parallel agents exploring different treatments and approaches to this question)\n",
    "    - Example 2: \"Compare the economic systems of three Nordic countries\" (benefits from simultaneous independent research on each country)\n",
    "\n",
    "3. **Develop a detailed outline plan**: Based on the question type, develop a detailed outline of your final response with clear sections. Each section should address a single sub-topic. The result should be the outline of an excellent answer to the user's question. Prioritize foundational understanding → core evidence → comparative analysis.\n",
    "\n",
    "  - For **straightforward queries**:\n",
    "    - Identify the most direct, efficient answer to the answer.\n",
    "    - Determine whether basic fact-finding or minor analysis is needed. If yes, define a specific sub-question you need to answer and the best available tool to use.\n",
    "\n",
    "  - For **deep research questions**:\n",
    "    - Define 3-5 different sub-questions or sub-topics that can be researched independently to answer the query comprehensively.\n",
    "    - List specific expert viewpoints or sources of evidence that would enrich the analysis and the best available tool to retrieve that information.\n",
    "    - Plan how findings will be aggregated into a coherent whole.\n",
    "    - Also include an Introduction and Conclusions section\n",
    "    - Example 1: For \"What causes obesity?\", the outline could include sections on genetic factors, environmental influences, psychological aspects, socioeconomic patterns, and biomedical evidence.\n",
    "    - Example 2: For \"Compare EU country tax systems\", the outline could include sections on what metrics and factors would be relevant to compare each country's tax systems and comparative analysis of those metrics and factors for the key countries in Northern Europe, Western Europe, Eastern Europe, Southern Europe.\n",
    "\n",
    "4. (Deep research questions only) **Save the outline**: Create a file in the current directory named `./outline.md` that documents the user question and the response outline. Make sure that IF all the outline sections are populated very well, THEN the results in aggregate would allow you to give an EXCELLENT answer to the user's question - complete, thorough, detailed, and accurate.\n",
    "\n",
    "  An example outline for the \"What causes obesity?\" question is:\n",
    "\n",
    "  # The Causes of Obesity\n",
    "\n",
    "  ## User Question\n",
    "\n",
    "  \"What causes obesity?\"\n",
    "\n",
    "  ## Outline\n",
    "\n",
    "  ### Introduction\n",
    "  ### Section 1: The genetic factors that could lead to obesity\n",
    "    - **Objective**:  \"What are the genetic factors linked to obesity?\"\n",
    "    - **Search Strategy**: [search terms]\n",
    "    - **Key Data**: [What to extract]\n",
    "  ### Section 2: The environmental factors that could lead to obesity\n",
    "    - **Objective**:  \"What environmental factors are associated with obesity and other metabolic conditions?\"\n",
    "    - **Search Strategy**: [search terms]\n",
    "    - **Key Data**: [What to extract]\n",
    "  ### Section 3:  ...\n",
    "  ### Conclusion\n",
    "\n",
    "4. (Deep research questions only) **Review the outline**: Share the outline with the user and ask for their questions or feedback. Update the outline based on their feedback and capture any additional information they share in the most appropriate section. Do not proceed until the user approves the outline.\n",
    "\n",
    "5. **Research**: Research the topics included in section 1 of the outline. Use the tools listed and your innate knowledge to answer any sub-questions or otherwise retrieve the necessary information. Once you have completed your research, update the outline with any evidence you have gathered and list the sources for this section.\n",
    "\n",
    "6. **Repeat**: Repeat the research step for all sections, updating the outline document as you go.\n",
    "\n",
    "7. **Review**: Before writing the final report, reflect on your research process. Does the outline fully address the user question? Is it complete, thorough, detailed, and accurate? If not, add one or more additional topics to the outline, execute them, and update the outline with the results.\n",
    "\n",
    "8. **Write the final report** When you have completed researching all sections of the outline, create a new file in the current directory named `./report.md` and write an excellent research report in paragraph format using the outline as your guide. Be sure to include all of the evidence you gathered and list the sources. \n",
    "\n",
    "</research_process>\n",
    "\"\"\"\n",
    "\n",
    "print(f'\\nThe current system prompt is:\\n\"\"\"{system_prompt}\"\"\"\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3e5730",
   "metadata": {},
   "source": [
    "### 2.4. Provide guidance on final report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt += \"\"\"\n",
    "<final_report>\n",
    "When generating your final research report, structure it as a comprehensive document that clearly communicates your research findings to the reader. Follow these guidelines:\n",
    "\n",
    "Report Structure:\n",
    "\n",
    "- Begin with a concise introduction (1-2 paragraphs) that establishes the research question, explains why it's important, and provides a brief overview of your approach\n",
    "- Organize the main body into sections that correspond to the major research tasks you completed (e.g., \"Literature Review,\" \"Current State Analysis,\" \"Comparative Assessment,\" \"Technical Evaluation,\" etc.)\n",
    "- Conclude with a summary section (1-2 paragraphs) that synthesizes key findings and discusses implications\n",
    "\n",
    "Section Format:\n",
    "\n",
    "- Write each section in paragraph format using 1-3 well-developed paragraphs\n",
    "- Each paragraph should focus on a coherent theme or finding\n",
    "- Use clear topic sentences and logical flow between paragraphs\n",
    "- Integrate information from multiple sources within paragraphs rather than listing findings separately\n",
    "\n",
    "Citation Requirements:\n",
    "\n",
    "- Include proper citations for all factual claims using the format provided in your source materials\n",
    "- Place citations at the end of sentences before punctuation (e.g., \"Recent studies show significant progress in this area .\")\n",
    "- Group related information from the same source under single citations when possible\n",
    "- Ensure every major claim is supported by appropriate source attribution\n",
    "\n",
    "Writing Style:\n",
    "\n",
    "- Use clear, professional academic language appropriate for scientific communication\n",
    "- Use active voice and strong verbs\n",
    "- Synthesize information rather than simply summarizing individual sources\n",
    "- Draw connections between different pieces of information and highlight patterns or contradictions\n",
    "- Focus on analysis and interpretation, not just information presentation\n",
    "- Don't use unnecessary words. Keep sentences short and concise.\n",
    "- WRite for a global audience. Avoid jargon an colloquial language. \n",
    "\n",
    "Quality Standards:\n",
    "\n",
    "- Ensure logical flow between sections and paragraphs\n",
    "- Maintain consistency in terminology and concepts throughout\n",
    "- Provide sufficient detail to support conclusions while remaining concise\n",
    "- End with actionable insights or clear implications based on your research findings </final_report>\n",
    "\n",
    "</final_report>\n",
    "\"\"\"\n",
    "\n",
    "print(f'\\nThe current system prompt is:\\n\"\"\"{system_prompt}\"\"\"\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90efa15a",
   "metadata": {},
   "source": [
    "## 3. Update Model Configuration\n",
    "\n",
    "From this point onward, our agent is going to start generating a lot of tokens. We'll need to adjust some configurations to handle this.\n",
    "\n",
    "### 3.1. Max Tokens\n",
    "\n",
    "Our agent uses tokens both to reason about user requests and also generate the research plan and report. We need to increase the `max_tokens` limit to avoid exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67caa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands.models import BedrockModel\n",
    "\n",
    "model = BedrockModel(\n",
    "    model_id=MODEL_ID,\n",
    "    max_tokens=10000,  # Increase the max tokens to accomodate the research plan and report generation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5768c5",
   "metadata": {},
   "source": [
    "### 3.2. Prompt Caching\n",
    "\n",
    "Prompt caching is an optional feature you can use with supported models on Amazon Bedrock to reduce inference latency and input token costs. It that allows you to cache long and repeated contexts that are frequently reused for multiple queries. When you cache portions of your prompt, the model can skip recomputation of those inputs, leading to faster responses and lower costs. This is particularly useful for scenarios like chatbots where users upload documents and ask multiple questions about them.\n",
    "\n",
    "For Claude models, Amazon Bedrock offers simplified cache management that automatically checks for cache hits at previous content block boundaries, looking back up to approximately 20 content blocks from your specified breakpoint. This reduces the complexity of manually placing cache checkpoints. \n",
    "\n",
    "This feature is particularly valuable for applications with repetitive, long contexts where the same information is processed multiple times across different user interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ae576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands.models import BedrockModel\n",
    "\n",
    "model = BedrockModel(\n",
    "    model_id=MODEL_ID,\n",
    "    max_tokens=10000,\n",
    "    cache_prompt=\"default\",  # Enables caching of the system prompt\n",
    "    cache_tools=\"default\",   # Enables caching of tool definitions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de989ce",
   "metadata": {},
   "source": [
    "### 3.3. Extended / Interleaved Thinking\n",
    "\n",
    "Extended thinking is a powerful feature that gives LLMs like Claude Sonnet enhanced reasoning capabilities for complex tasks by allowing it to work through problems step-by-step before delivering a final answer. When extended thinking is enabled, Claude creates internal reasoning blocks where it outputs its thought process, then incorporates insights from this reasoning to craft a more thoughtful final response. \n",
    "\n",
    "Interleaved thinking is a particularly sophisticated feature available in Claude 4 models that enables Claude to think between tool calls and make more nuanced decisions based on intermediate results. This specifically enables:\n",
    "\n",
    "- Dynamic Reasoning: Claude can reason about tool results before deciding what to do next\n",
    "- Chain Tool Calls: Multiple tool calls can be linked with reasoning steps in between\n",
    "- Adaptive Planning: The model can adjust its approach dynamically based on intermediate results \n",
    "\n",
    "The main difference you'll notice with the interleaved thinking is that Event loop is acting on LLM's \"thoughts\", rather than \"decisions\".  In a traditional event loop, the thoughts are hidden. We have to wait until LLM renders either a decision to call a tool or produces the Final Answer. \n",
    "\n",
    "In case of interleaved thinking, LLM is \"leaking\" its thoughts into the even loop while it's still in that second step - \"LLM is thinking\" - and event loop is configured to executed the tools as soon as LLM \"thinks\" about doing it. What this means is that by the time LLM is done thinking, it actually has the Final Answer, on the very first \"decision\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd8884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands.models import BedrockModel\n",
    "\n",
    "model = BedrockModel(\n",
    "    model_id=MODEL_ID,\n",
    "    max_tokens=10000,\n",
    "    cache_prompt=\"default\",\n",
    "    cache_tools=\"default\",\n",
    "    temperature=1,  # Required to be 1 when thinking is enabled\n",
    "    additional_request_fields={\n",
    "        # Enable interleaved thinking beta feature\n",
    "        \"anthropic_beta\": [\"interleaved-thinking-2025-05-14\"],\n",
    "        # Configure reasoning parameters\n",
    "        \"reasoning_config\": {\n",
    "            \"type\": \"enabled\",  # Turn on thinking\n",
    "            \"budget_tokens\": 3000,  # Thinking token budget\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3199c141",
   "metadata": {},
   "source": [
    "Now our model should have sufficient token capacity to complete the mock planning and report generation process. It still doesn't have access to any tools, so the information presented will only be from it's own knowledge and likely contain inaccuracies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817ac001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "\n",
    "agent = Agent(model=model, system_prompt=system_prompt)\n",
    "response = agent(\"How safe and effective are GLP-1 drugs for long term use?\")\n",
    "response = agent(\n",
    "    \"Please proceed with the first section only and then immediately draft the final report. You may proceed when ready.\"\n",
    ")\n",
    "\n",
    "response.metrics.accumulated_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f3065",
   "metadata": {},
   "source": [
    "## 4. Full Deep Research Agent with tools\n",
    "\n",
    "We want our agent to create and update files throughout the research process, so we'll need to give it the tools to do so. Our full deep research agent will require access to three tools:\n",
    "\n",
    "1. `search_pmc_tool` from notebook 1 for searching PubMed Central (PMC) for relevant scientific articile abstracts\n",
    "2. `gather_evidence_tool` from notebook 2 for gathering detailed evidence from full-text articles using the PaperQA2 tool\n",
    "3. The pre-built Strands `editor` tool for writing and updating files.\n",
    "\n",
    "Let's add them to our agent and start a test run. This will take several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ab324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from strands.models import BedrockModel\n",
    "from strands_tools import editor\n",
    "from search_pmc import search_pmc_tool\n",
    "from gather_evidence import gather_evidence_tool\n",
    "import os\n",
    "\n",
    "model = BedrockModel(\n",
    "    model_id=MODEL_ID,\n",
    "    max_tokens=10000,\n",
    "    cache_prompt=\"default\",\n",
    "    cache_tools=\"default\",\n",
    "    temperature=1,\n",
    "    additional_request_fields={\n",
    "        \"anthropic_beta\": [\"interleaved-thinking-2025-05-14\"],\n",
    "        \"reasoning_config\": {\n",
    "            \"type\": \"enabled\",\n",
    "            \"budget_tokens\": 3000,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "os.environ[\"BYPASS_TOOL_CONSENT\"] = \"true\"\n",
    "\n",
    "agent = Agent(\n",
    "    model=model,\n",
    "    system_prompt=system_prompt,\n",
    "    tools=[editor, search_pmc_tool, gather_evidence_tool],\n",
    ")\n",
    "response = agent(\"How safe and effective are GLP-1 drugs for long term use?\")\n",
    "response = agent(\n",
    "    \"Please proceed with researching the first section only and then immediately draft the final report. You may proceed when ready.\"\n",
    ")\n",
    "\n",
    "response.metrics.accumulated_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e49d04",
   "metadata": {},
   "source": [
    "In this case, we limited the research to only section 1. However, the agent was able to retrieve relevant information and generate a high-quality report with citations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bed05e",
   "metadata": {},
   "source": [
    "## 5. Deploy to Amazon Bedrock AgentCore Runtime\n",
    "\n",
    "Let's look at the new agent definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017cbfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1b980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from bedrock_agentcore_starter_toolkit import Runtime\n",
    "\n",
    "ssm = boto3.client(\"ssm\")\n",
    "\n",
    "agentcore_runtime = Runtime()\n",
    "agentcore_runtime.configure(\n",
    "    agent_name=\"pmc_deep_research_agent\",\n",
    "    auto_create_ecr=True,\n",
    "    execution_role=ssm.get_parameter(\n",
    "        Name=\"/deep-research-workshop/agentcore-runtime-role-arn\"\n",
    "    )[\"Parameter\"][\"Value\"],\n",
    "    entrypoint=\"agent.py\",\n",
    "    memory_mode=\"NO_MEMORY\",\n",
    "    requirements_file=\"requirements.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c6517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentcore_runtime.launch(auto_update_on_conflict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce86d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "session_id = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from invoke_agentcore import invoke_agentcore\n",
    "invoke_agentcore(\n",
    "    agent_runtime_name=\"pmc_deep_research_agent\",\n",
    "    prompt=\"How safe and effective are GLP-1 drugs for long term use?\",\n",
    "    session_id = session_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17920ea5",
   "metadata": {},
   "source": [
    "## 6. (Optional) Interact with agent using AgentCore Chat\n",
    "\n",
    "Follow these steps to open an interactive chat session with your new agent.\n",
    "\n",
    "1. Open a command line terminal in your notebook environment.\n",
    "2. Navigate to the project root folder (where `pyproject.toml` is located).\n",
    "3. Run `pip install .` to install the workshop tools including the chat CLI.\n",
    "4. Run `agentcore-chat` to launch the CLI.\n",
    "5. Select the `pmc_deep_research_agent` by typing its name or index in the terminal and press Enter.\n",
    "6. Ask your question at the `You:` prompt and press Enter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2554dc97",
   "metadata": {},
   "source": [
    "## 7. (Optional) Clean Up\n",
    "\n",
    "Run the next notebook cell to delete the AgentCore runtime environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea5334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "agentcore_client = boto3.client(\"bedrock-agentcore-control\")\n",
    "agent_status = agentcore_runtime.status()\n",
    "\n",
    "agentcore_client.delete_agent_runtime(agentRuntimeId=agent_status.config.agent_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
