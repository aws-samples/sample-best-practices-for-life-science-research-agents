{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d534971-cb20-4752-9331-a3f98339eced",
   "metadata": {},
   "source": [
    "# Gather Evidence Using PaperQA2\n",
    "\n",
    "In this notebook, you'll use the PaperQA2 library from FutureHouse to gather evidence from PubMedCentral (PMC) articles.\n",
    "\n",
    "## 1. Prerequisites\n",
    "\n",
    "- Python 3.10 or later\n",
    "- AWS account configured with appropriate permissions\n",
    "- Access to the Anthropic Claude Sonnet 4 model on Amazon Bedrock\n",
    "- Basic understanding of Python programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722465ef-f70b-48fc-9b78-d4d8a49f30b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%pip install -U boto3 strands-agents strands-agents-tools defusedxml httpx bedrock_agentcore_starter_toolkit paper-qa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96912844-0b45-43e6-a233-286045130e7d",
   "metadata": {},
   "source": [
    "## 2. Experiment with PaperQA2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea721cf0",
   "metadata": {},
   "source": [
    "### 2.1. Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a5595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0122dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperqa import Settings, ask\n",
    "from paperqa.settings import AgentSettings, ParsingSettings, IndexSettings\n",
    "\n",
    "LLM = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "\n",
    "answer_response = await ask(\n",
    "    \"What is PaperQA2?\",\n",
    "    settings=Settings(\n",
    "        llm=LLM,\n",
    "        summary_llm=LLM,\n",
    "        agent=AgentSettings(\n",
    "            agent_llm=LLM,\n",
    "            index=IndexSettings(index_directory=\"indexes\"),\n",
    "        ),\n",
    "        embedding=\"bedrock/amazon.titan-embed-text-v2:0\",\n",
    "        paper_directory=\"my_papers\",\n",
    "        parsing=ParsingSettings(use_doc_details=False),\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e05c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_paperqa_results(answer) -> None:\n",
    "    \"\"\"Pretty print the output from the PaperQA2 query.\"\"\"\n",
    "    session_output = answer.session\n",
    "\n",
    "    print(\"**Question**\\n\")\n",
    "    print(session_output.question + \"\\n\")\n",
    "    print(f\"**Answer**\\n\")\n",
    "    print(session_output.answer + \"\\n\")\n",
    "    print(\"**Evidence**\\n\")\n",
    "    for context in session_output.contexts:\n",
    "        print(f\"{context.text.name}:\\t{context.context}\")\n",
    "    print(\"**Token Counts**\\n\")\n",
    "    print(f\"{'Model':<45} {'Input':<8} {'Output':<8}\")\n",
    "    print(\"-\" * 65)\n",
    "    for model, values in session_output.token_counts.items():\n",
    "        print(f\"{model:<45} {values[0]:<8} {values[1]:<8}\")\n",
    "    print()\n",
    "    print(\"**Estimated Cost**\\n\")\n",
    "    print(round(session_output.cost, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563ae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_paperqa_results(answer_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86a8f85",
   "metadata": {},
   "source": [
    "### 2.2. Cost Optimization with Multiple LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeac69f",
   "metadata": {},
   "source": [
    "Try different llms. From the docs:\n",
    "\n",
    "- `llm`: LLM for general use including metadata inference and answer generation. Should be 'best' LLM. Uses include:\n",
    "    1. Inferring citation information from documents when left unspecified\n",
    "    1. Extracting title, DOI, and authors from citation information when left unspecified\n",
    "    1. Optionally injecting pre-answer information\n",
    "    1. Generating an answer given evidence\n",
    "    1. Optionally injecting post-answer information \n",
    "- `summary_llm`: LLM for creating contextual summaries \n",
    "- `agent_llm`: LLM inside the agent making tool selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9cf32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperqa import Settings, ask\n",
    "from paperqa.settings import AgentSettings, ParsingSettings, IndexSettings\n",
    "\n",
    "LLM = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "SUMMARY_LLM = \"bedrock/openai.gpt-oss-120b-1:0\"\n",
    "\n",
    "answer_response = await ask(\n",
    "    \"What is PaperQA2?\",\n",
    "    settings=Settings(\n",
    "        llm=LLM,\n",
    "        summary_llm=SUMMARY_LLM,\n",
    "        agent=AgentSettings(\n",
    "            agent_llm=LLM,\n",
    "            index=IndexSettings(index_directory=\"indexes\"),\n",
    "        ),\n",
    "        embedding=\"bedrock/amazon.titan-embed-text-v2:0\",\n",
    "        paper_directory=\"my_papers\",\n",
    "        parsing=ParsingSettings(use_doc_details=False),\n",
    "    ),\n",
    ")\n",
    "\n",
    "pretty_print_paperqa_results(answer_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afcef46",
   "metadata": {},
   "source": [
    "By using a different model for the evidence summarization step, we were able to reduce the estimated cost by nearly half!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fdc510",
   "metadata": {},
   "source": [
    "### 2.3. Biomedical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1ad0b7",
   "metadata": {},
   "source": [
    "Use PMC11231910 as an example. First, download the text from RODA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a642a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import os\n",
    "\n",
    "s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "pmc_id = \"PMC9438179\"\n",
    "bucket_name = \"pmc-oa-opendata\"\n",
    "object_key = f\"oa_comm/txt/all/{pmc_id}.txt\"\n",
    "local_file_path = f\"my_papers/{pmc_id}.txt\"\n",
    "s3.download_file(bucket_name, object_key, local_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperqa import Settings, ask\n",
    "from paperqa.settings import AgentSettings, ParsingSettings, IndexSettings\n",
    "\n",
    "LLM = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "SUMMARY_LLM = \"bedrock/openai.gpt-oss-120b-1:0\"\n",
    "\n",
    "answer_response = await ask(\n",
    "    \"How safe and effective are GLP-1 drugs for long term use?\",\n",
    "    settings=Settings(\n",
    "        llm=LLM,\n",
    "        summary_llm=SUMMARY_LLM,\n",
    "        agent=AgentSettings(\n",
    "            agent_llm=LLM,\n",
    "            index=IndexSettings(index_directory=\"indexes\"),\n",
    "        ),\n",
    "        embedding=\"bedrock/amazon.titan-embed-text-v2:0\",\n",
    "        paper_directory=\"my_papers\",\n",
    "        parsing=ParsingSettings(use_doc_details=False),\n",
    "    ),\n",
    ")\n",
    "\n",
    "pretty_print_paperqa_results(answer_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2831a-cad8-4c41-a0e5-9503e1a503f4",
   "metadata": {},
   "source": [
    "## 4. Create Strands Agent and tool\n",
    "\n",
    "There's an example of how to add PaperQA to a Strands tool in `gather_evidence.py`. Let's incorporate it into a test agent and call it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from gather_evidence import gather_evidence_tool\n",
    "\n",
    "MODEL_ID = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "agent = Agent(tools=[gather_evidence_tool], model=MODEL_ID)\n",
    "\n",
    "agent.tool.gather_evidence_tool(pmcid=\"PMC9438179\", question=\"How safe and effective are GLP-1 drugs for long term use?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b0d4f4",
   "metadata": {},
   "source": [
    "## 5. Deploy to Amazon Bedrock AgentCore Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7541b4f1",
   "metadata": {},
   "source": [
    "## 6. (Optional) Interact with agent using AgentCore Chat\n",
    "\n",
    "Follow these steps to open an interactive chat session with your new agent.\n",
    "\n",
    "1. Open a command line terminal in your notebook environment.\n",
    "2. Navigate to the project root folder (where `pyproject.toml` is located).\n",
    "3. Run `pip install .` to install the workshop tools including the chat CLI.\n",
    "4. Run `agentcore-chat` to launch the CLI.\n",
    "5. Select the `pmc_abstract_agent` by typing its name or index in the terminal and press Enter.\n",
    "6. Ask your question at the `You:` prompt and press Enter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd841b58",
   "metadata": {},
   "source": [
    "## 7. (Optional) Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35fde05",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
