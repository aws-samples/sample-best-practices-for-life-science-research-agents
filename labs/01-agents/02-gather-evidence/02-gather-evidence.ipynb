{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d534971-cb20-4752-9331-a3f98339eced",
   "metadata": {},
   "source": [
    "# Gather Evidence Using PaperQA2\n",
    "\n",
    "![Agent with scientific knowledge tool](../../../static/agent-gather-evidence.png)\n",
    "\n",
    "The lab introduces sophisticated RAG techniques including\n",
    "\n",
    "- Re-ranking of retrieved passages for relevance\n",
    "- Contextual summarization of evidence\n",
    "- Precise citation tracking with token count monitoring\n",
    "- Cost estimation for different model configurations\n",
    "\n",
    "This lab uses PaperQA2, a sophisticated retrieval-augmented generation (RAG) agent developed by FutureHouse that specializes in scientific literature analysis. It uses a technique called \"reranking and contextual summarization\" or RCS to improve the quality of retrieved evidence. This proces begins with a standard top-k dense vector rerieval step using embedding vectors to identify potentially relevant document chunks. High-scoring chunks are then summarized using a LLM and reranked to ensure that only the most relevant text influences the final answer. PaperQA2 demonstrated superior performance, achieving 85.2% precision in question answering tasks compared to 73.8% human performance [1].\n",
    "\n",
    "In this lab, you will explore the impact of advanced RAG techniques on the quality of the agentic response. You will also experiment with different LLMs and observe the impact on output quality and inference cost. Finally you will build a incorporate PaperQA2 into a `generate_evidence` tool and add it to a Strands agent deployed on Bedrock AgentCore.\n",
    "\n",
    "## Sources\n",
    "\n",
    "1. Skarlinski, Michael D., et al. \"Language agents achieve superhuman synthesis of scientific knowledge.\" arXiv preprint arXiv:2409.13740, 26 Sept. 2024, doi.org/10.48550/arXiv.2409.13740.\n",
    "\n",
    "## 1. Prerequisites\n",
    "\n",
    "- Python 3.10 or later\n",
    "- AWS account configured with appropriate permissions\n",
    "- Access to the Anthropic Claude Sonnet 4 model on Amazon Bedrock\n",
    "- Basic understanding of Python programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722465ef-f70b-48fc-9b78-d4d8a49f30b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -U boto3 strands-agents strands-agents-tools defusedxml httpx bedrock_agentcore_starter_toolkit paper-qa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96912844-0b45-43e6-a233-286045130e7d",
   "metadata": {},
   "source": [
    "## 2. Experiment with PaperQA2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea721cf0",
   "metadata": {},
   "source": [
    "### 2.1. Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a5595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic\")\n",
    "warnings.filterwarnings(\"ignore\", module=\"litellm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776787c7",
   "metadata": {},
   "source": [
    "Download example paper Rodriguez, Patricia J., et al. \"Semaglutide vs Tirzepatide for Weight Loss in Adults With Overweight or Obesity.\" JAMA Internal Medicine, vol. 184, no. 9, 8 July 2024, pp. 1056-1064, doi:10.1001/jamainternmed.2024.2525.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ecd341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "pmc_id = \"PMC9438179\"\n",
    "bucket_name = \"pmc-oa-opendata\"\n",
    "object_key = f\"oa_comm/txt/all/{pmc_id}.txt\"\n",
    "\n",
    "# Create papers directory if it doesn't exist\n",
    "papers_dir = Path(f\"my_papers/{pmc_id}/txt\")\n",
    "papers_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "local_file_path = f\"my_papers/{pmc_id}/txt/{pmc_id}.txt\"\n",
    "s3.download_file(bucket_name, object_key, local_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0122dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperqa import Settings, ask\n",
    "from paperqa.settings import (\n",
    "    AgentSettings,\n",
    "    ParsingSettings,\n",
    "    IndexSettings,\n",
    "    AnswerSettings,\n",
    ")\n",
    "\n",
    "LLM = \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "SUMMARY_LLM = \"bedrock/openai.gpt-oss-120b-1:0\"\n",
    "AGENT_TYPE = \"fake\"\n",
    "EVIDENCE_K = 5\n",
    "EVIDENCE_SUMMARY_LENGTH = \"25 to 50 words\"\n",
    "\n",
    "answer_response = await ask(\n",
    "    \"How safe and effective are GLP-1 drugs for long term use?\",\n",
    "    settings=Settings(\n",
    "        llm=LLM,\n",
    "        summary_llm=SUMMARY_LLM,\n",
    "        agent=AgentSettings(\n",
    "            agent_llm=LLM,\n",
    "            index=IndexSettings(\n",
    "                index_directory=\"my_papers/PMC9438179/index\",\n",
    "                paper_directory=\"my_papers/PMC9438179/txt\",\n",
    "            ),\n",
    "            agent_type=AGENT_TYPE,\n",
    "        ),\n",
    "        embedding=\"bedrock/amazon.titan-embed-text-v2:0\",\n",
    "        parsing=ParsingSettings(use_doc_details=False),\n",
    "        answer=AnswerSettings(\n",
    "            answer_max_sources=1,\n",
    "            evidence_k=EVIDENCE_K,\n",
    "            evidence_summary_length=EVIDENCE_SUMMARY_LENGTH,\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e05c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_paperqa_results(answer) -> None:\n",
    "    \"\"\"Pretty print the output from the PaperQA2 query.\"\"\"\n",
    "    session_output = answer.session\n",
    "\n",
    "    print(\"**Question**\\n\")\n",
    "    print(session_output.question + \"\\n\")\n",
    "    print(\"**Answer**\\n\")\n",
    "    print(session_output.answer + \"\\n\")\n",
    "    print(\"**Evidence**\\n\")\n",
    "    for context in session_output.contexts:\n",
    "        print(f\"{context.text.name}:\\t{context.context}\")\n",
    "    print(\"**Token Counts**\\n\")\n",
    "    print(f\"{'Model':<45} {'Input':<8} {'Output':<8}\")\n",
    "    print(\"-\" * 65)\n",
    "    for model, values in session_output.token_counts.items():\n",
    "        print(f\"{model:<45} {values[0]:<8} {values[1]:<8}\")\n",
    "    print()\n",
    "    print(\"**Estimated Cost**\\n\")\n",
    "    print(round(session_output.cost, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563ae5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretty_print_paperqa_results(answer_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86a8f85",
   "metadata": {},
   "source": [
    "### 2.2. Cost Optimization with Multiple LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeac69f",
   "metadata": {},
   "source": [
    "Try different llms. From the docs:\n",
    "\n",
    "- `llm`: LLM for general use including metadata inference and answer generation. Should be 'best' LLM. Uses include:\n",
    "    1. Inferring citation information from documents when left unspecified\n",
    "    1. Extracting title, DOI, and authors from citation information when left unspecified\n",
    "    1. Optionally injecting pre-answer information\n",
    "    1. Generating an answer given evidence\n",
    "    1. Optionally injecting post-answer information \n",
    "- `summary_llm`: LLM for creating contextual summaries \n",
    "- `agent_llm`: LLM inside the agent making tool selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9cf32f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from paperqa import Settings, ask\n",
    "from paperqa.settings import (\n",
    "    AgentSettings,\n",
    "    ParsingSettings,\n",
    "    IndexSettings,\n",
    "    AnswerSettings,\n",
    ")\n",
    "\n",
    "LLM = \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "SUMMARY_LLM = \"bedrock/openai.gpt-oss-120b-1:0\"\n",
    "AGENT_TYPE = \"fake\"\n",
    "EVIDENCE_K = 5\n",
    "EVIDENCE_SUMMARY_LENGTH = \"25 to 50 words\"\n",
    "\n",
    "answer_response = await ask(\n",
    "    \"What is PaperQA?\",\n",
    "    settings=Settings(\n",
    "        llm=LLM,\n",
    "        summary_llm=SUMMARY_LLM,\n",
    "        agent=AgentSettings(\n",
    "            agent_llm=LLM,\n",
    "            index=IndexSettings(\n",
    "                index_directory=\"my_papers/paperqa/index\",\n",
    "                paper_directory=\"my_papers/paperqa/txt\",\n",
    "            ),\n",
    "            agent_type=AGENT_TYPE,\n",
    "        ),\n",
    "        embedding=\"bedrock/amazon.titan-embed-text-v2:0\",\n",
    "        parsing=ParsingSettings(use_doc_details=False),\n",
    "        answer=AnswerSettings(\n",
    "            answer_max_sources=1,\n",
    "            evidence_k=EVIDENCE_K,\n",
    "            evidence_summary_length=\"about 50 words\",\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "pretty_print_paperqa_results(answer_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afcef46",
   "metadata": {},
   "source": [
    "By using a different model for the evidence summarization step, we were able to reduce the estimated cost by nearly half!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2831a-cad8-4c41-a0e5-9503e1a503f4",
   "metadata": {},
   "source": [
    "## 3. Create Strands Agent and tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d9b43",
   "metadata": {},
   "source": [
    "### 3.1. Call Gather Evidence Tool Directly\n",
    "\n",
    "There's an example of how to add PaperQA to a Strands tool in `gather_evidence.py`. Let's incorporate it into a test agent and call it directly. This may take 1-2 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from gather_evidence import gather_evidence_tool\n",
    "\n",
    "MODEL_ID = \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "agent = Agent(tools=[gather_evidence_tool], model=MODEL_ID)\n",
    "\n",
    "agent.tool.gather_evidence_tool(\n",
    "    pmcid=\"PMC9438179\",\n",
    "    question=\"How safe and effective are GLP-1 drugs for long term use?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c0dea",
   "metadata": {},
   "source": [
    "### 3.2. Define Agent\n",
    "\n",
    "Let's define an agent that can use the `search_pmc` tool from notebook 01 and the new `gather_evidence` tool from this notebook to find and understand scientific articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fe2ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from search_pmc import search_pmc_tool\n",
    "from gather_evidence import gather_evidence_tool\n",
    "\n",
    "MODEL_ID = \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "QUERY = \"How safe and effective are GLP-1 drugs for long term use?\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a life science research assistant. When given a scientific question, follow this process:\n",
    "\n",
    "1. Use search_pmc_tool to find highly-cited papers. Search broadly first, then narrow down. Use temporal filters like \"last 2 years\"[dp] for recent work. \n",
    "2. Identify the PMC ID value for the 2 relevant papers, then submit their ID values and the query to the gather_evidence_tool.\n",
    "3. Generate a concise answer to the question based on the most relevant evidence, along with source citations\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Initialize your agent\n",
    "agent = Agent(\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    tools=[search_pmc_tool, gather_evidence_tool],\n",
    "    model=MODEL_ID,\n",
    ")\n",
    "\n",
    "# Send a message to the agent\n",
    "response = agent(QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b0d4f4",
   "metadata": {},
   "source": [
    "## 4. Deploy to Amazon Bedrock AgentCore Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130078be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from bedrock_agentcore_starter_toolkit import Runtime\n",
    "\n",
    "ssm = boto3.client(\"ssm\")\n",
    "\n",
    "agentcore_runtime = Runtime()\n",
    "agentcore_runtime.configure(\n",
    "    agent_name=\"pmc_gather_evidence_agent\",\n",
    "    auto_create_ecr=True,\n",
    "    execution_role=ssm.get_parameter(\n",
    "        Name=\"/deep-research-workshop/agentcore-runtime-role-arn\"\n",
    "    )[\"Parameter\"][\"Value\"],\n",
    "    entrypoint=\"agent.py\",\n",
    "    memory_mode=\"NO_MEMORY\",\n",
    "    requirements_file=\"requirements.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a982fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentcore_runtime.launch(auto_update_on_conflict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1707e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "agentcore_runtime.invoke(\n",
    "    {\"prompt\": \"How safe and effective are GLP-1 drugs for long term use?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7541b4f1",
   "metadata": {},
   "source": [
    "## 5. (Optional) Interact with agent using AgentCore Chat\n",
    "\n",
    "Follow these steps to open an interactive chat session with your new agent.\n",
    "\n",
    "1. Open a command line terminal in your notebook environment.\n",
    "2. Navigate to the project root folder (where `pyproject.toml` is located).\n",
    "3. Run `pip install .` to install the workshop tools including the chat CLI.\n",
    "4. Run `agentcore-chat` to launch the CLI.\n",
    "5. Select the `pmc_evidence_agent` by typing its name or index in the terminal and press Enter.\n",
    "6. Ask your question at the `You:` prompt and press Enter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd841b58",
   "metadata": {},
   "source": [
    "## 6. (Optional) Clean Up\n",
    "\n",
    "Run the next notebook cell to delete the AgentCore runtime environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35fde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "agentcore_client = boto3.client(\"bedrock-agentcore-control\")\n",
    "agent_status = agentcore_runtime.status()\n",
    "\n",
    "agentcore_client.delete_agent_runtime(agentRuntimeId=agent_status.config.agent_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
