{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d534971-cb20-4752-9331-a3f98339eced",
   "metadata": {},
   "source": [
    "# Gather Evidence Using PaperQA2\n",
    "\n",
    "![Agent with scientific knowledge tool](../../../static/agent-gather-evidence.png)\n",
    "\n",
    "The lab introduces sophisticated RAG techniques including\n",
    "\n",
    "- Re-ranking of retrieved passages for relevance\n",
    "- Contextual summarization of evidence\n",
    "- Precise citation tracking with token count monitoring\n",
    "- Cost estimation for different model configurations\n",
    "\n",
    "This lab uses PaperQA2, a sophisticated retrieval-augmented generation (RAG) agent developed by FutureHouse that specializes in scientific literature analysis. It uses a technique called \"reranking and contextual summarization\" or RCS to improve the quality of retrieved evidence. This proces begins with a standard top-k dense vector rerieval step using embedding vectors to identify potentially relevant document chunks. High-scoring chunks are then summarized using a LLM and reranked to ensure that only the most relevant text influences the final answer. PaperQA2 demonstrated superior performance, achieving 85.2% precision in question answering tasks compared to 73.8% human performance [1].\n",
    "\n",
    "In this lab, you will explore the impact of advanced RAG techniques on the quality of the agentic response. You will also experiment with different LLMs and observe the impact on output quality and inference cost. Finally you will build a incorporate PaperQA2 into a `generate_evidence` tool and add it to a Strands agent deployed on Bedrock AgentCore.\n",
    "\n",
    "## Sources\n",
    "\n",
    "1. Skarlinski, Michael D., et al. \"Language agents achieve superhuman synthesis of scientific knowledge.\" arXiv preprint arXiv:2409.13740, 26 Sept. 2024, doi.org/10.48550/arXiv.2409.13740.\n",
    "\n",
    "## 1. Prerequisites\n",
    "\n",
    "- Python 3.10 or later\n",
    "- AWS account configured with appropriate permissions\n",
    "- Access to the Anthropic Claude Sonnet 4 model on Amazon Bedrock\n",
    "- Basic understanding of Python programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722465ef-f70b-48fc-9b78-d4d8a49f30b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -U boto3 strands-agents strands-agents-tools defusedxml httpx bedrock_agentcore_starter_toolkit paper-qa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96912844-0b45-43e6-a233-286045130e7d",
   "metadata": {},
   "source": [
    "## 2. Experiment with PaperQA2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea721cf0",
   "metadata": {},
   "source": [
    "### 2.1. Basic usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776787c7",
   "metadata": {},
   "source": [
    "Start by downloading download an example paper from PubMed Central: \n",
    "\n",
    "Rodriguez, Patricia J., et al. \"Semaglutide vs Tirzepatide for Weight Loss in Adults With Overweight or Obesity.\" JAMA Internal Medicine, vol. 184, no. 9, 8 July 2024, pp. 1056-1064, doi:10.1001/jamainternmed.2024.2525.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ecd341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "pmc_id = \"PMC9438179\"\n",
    "bucket_name = \"pmc-oa-opendata\"\n",
    "object_key = f\"oa_comm/txt/all/{pmc_id}.txt\"\n",
    "\n",
    "# Create papers directory if it doesn't exist\n",
    "papers_dir = Path(f\"my_papers/{pmc_id}/txt\")\n",
    "papers_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "local_file_path = f\"my_papers/{pmc_id}/txt/{pmc_id}.txt\"\n",
    "s3.download_file(bucket_name, object_key, local_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0122dea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helpers import pretty_print_paperqa_results\n",
    "from paperqa import Settings, ask\n",
    "from paperqa.settings import (\n",
    "    AgentSettings,\n",
    "    ParsingSettings,\n",
    "    IndexSettings,\n",
    "    AnswerSettings,\n",
    ")\n",
    "\n",
    "LLM = \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "\n",
    "answer_response = await ask(\n",
    "    \"How safe and effective are GLP-1 drugs for long term use?\",\n",
    "    settings=Settings(\n",
    "        llm=LLM,\n",
    "        summary_llm=LLM,\n",
    "        agent=AgentSettings(\n",
    "            agent_llm=LLM,\n",
    "            index=IndexSettings(\n",
    "                index_directory=\"my_papers/PMC9438179/index\",\n",
    "                paper_directory=\"my_papers/PMC9438179/txt\",\n",
    "            ),\n",
    "        ),\n",
    "        embedding=\"bedrock/amazon.titan-embed-text-v2:0\",\n",
    "        parsing=ParsingSettings(use_doc_details=False),\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "pretty_print_paperqa_results(answer_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86a8f85",
   "metadata": {},
   "source": [
    "### 2.2. Cost Optimization with Multiple LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeac69f",
   "metadata": {},
   "source": [
    "PaperQA supports a variety of settings to optimize price and performance. In particular, you can use different LLMs for various tasks:\n",
    "\n",
    "- `llm`: LLM for general use including metadata inference and answer generation. Should be 'best' LLM. Uses include:\n",
    "    1. Inferring citation information from documents when left unspecified\n",
    "    1. Extracting title, DOI, and authors from citation information when left unspecified\n",
    "    1. Optionally injecting pre-answer information\n",
    "    1. Generating an answer given evidence\n",
    "    1. Optionally injecting post-answer information \n",
    "- `summary_llm`: LLM for creating contextual summaries \n",
    "- `agent_llm`: LLM inside the agent making tool selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9cf32f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helpers import pretty_print_paperqa_results\n",
    "from paperqa import Settings, ask\n",
    "from paperqa.settings import (\n",
    "    AgentSettings,\n",
    "    ParsingSettings,\n",
    "    IndexSettings,\n",
    "    AnswerSettings,\n",
    ")\n",
    "\n",
    "LLM = \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "SUMMARY_LLM = \"bedrock/openai.gpt-oss-120b-1:0\"\n",
    "AGENT_TYPE = \"fake\"\n",
    "EVIDENCE_K = 5\n",
    "EVIDENCE_SUMMARY_LENGTH = \"25 to 50 words\"\n",
    "\n",
    "answer_response = await ask(\n",
    "    \"What is PaperQA?\",\n",
    "    settings=Settings(\n",
    "        llm=LLM,\n",
    "        summary_llm=SUMMARY_LLM,\n",
    "        agent=AgentSettings(\n",
    "            agent_llm=LLM,\n",
    "            index=IndexSettings(\n",
    "                index_directory=\"my_papers/paperqa/index\",\n",
    "                paper_directory=\"my_papers/paperqa/txt\",\n",
    "            ),\n",
    "            agent_type=AGENT_TYPE,\n",
    "        ),\n",
    "        embedding=\"bedrock/amazon.titan-embed-text-v2:0\",\n",
    "        parsing=ParsingSettings(use_doc_details=False),\n",
    "        answer=AnswerSettings(\n",
    "            answer_max_sources=1,\n",
    "            evidence_k=EVIDENCE_K,\n",
    "            evidence_summary_length=\"about 50 words\",\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "pretty_print_paperqa_results(answer_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afcef46",
   "metadata": {},
   "source": [
    "By using a different model for the evidence summarization step, we were able to significantly reduce the estimated cost!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2831a-cad8-4c41-a0e5-9503e1a503f4",
   "metadata": {},
   "source": [
    "## 3. Create Strands Agent and tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d9b43",
   "metadata": {},
   "source": [
    "### 3.1. Call Gather Evidence Tool Directly\n",
    "\n",
    "There's an example of how to add PaperQA to a Strands tool in `gather_evidence.py`. Try it out directly and look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e121c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from gather_evidence import gather_evidence_tool\n",
    "\n",
    "MODEL_ID = \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "agent = Agent(tools=[gather_evidence_tool], model=MODEL_ID)\n",
    "\n",
    "agent.tool.gather_evidence_tool(\n",
    "    pmcid=\"PMC9438179\",\n",
    "    question=\"How safe and effective are GLP-1 drugs for long term use?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec441b3-bb31-4c73-940b-2675568f9764",
   "metadata": {},
   "source": [
    "The `gather_evidence` tool returns a structured evidence record with some helpful information:\n",
    "\n",
    "- The user question\n",
    "- The source (URL for the paper in PMC)\n",
    "- The summarized chunks retrieved from the document text\n",
    "- The generated answer\n",
    "\n",
    "This is much more comprehensive than the article abstract alone!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c0dea",
   "metadata": {},
   "source": [
    "### 3.2. Define Agent\n",
    "\n",
    "Let's define an agent that can use the `search_pmc` tool from notebook 01 and a new `gather_evidence` tool from this notebook to find and understand scientific articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fe2ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from search_pmc import search_pmc_tool\n",
    "from gather_evidence import gather_evidence_tool\n",
    "\n",
    "MODEL_ID = \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "QUERY = \"How safe and effective are GLP-1 drugs for long term use?\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a life science research assistant. When given a scientific question, follow this process:\n",
    "\n",
    "1. Use search_pmc_tool to find highly-cited papers. Search broadly first, then narrow down. Use temporal filters like \"last 2 years\"[dp] for recent work. \n",
    "2. Identify the PMC ID value for the 2 relevant papers, then submit their ID values and the query to the gather_evidence_tool.\n",
    "3. Generate a concise answer to the question based on the most relevant evidence, along with source citations\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Initialize your agent\n",
    "agent = Agent(\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    tools=[search_pmc_tool, gather_evidence_tool],\n",
    "    model=MODEL_ID,\n",
    ")\n",
    "\n",
    "# Send a message to the agent\n",
    "response = agent(QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b0d4f4",
   "metadata": {},
   "source": [
    "## 4. Deploy to Amazon Bedrock AgentCore Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130078be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from bedrock_agentcore_starter_toolkit import Runtime\n",
    "\n",
    "ssm = boto3.client(\"ssm\")\n",
    "\n",
    "agentcore_runtime = Runtime()\n",
    "agentcore_runtime.configure(\n",
    "    agent_name=\"pmc_gather_evidence_agent\",\n",
    "    auto_create_ecr=True,\n",
    "    execution_role=ssm.get_parameter(\n",
    "        Name=\"/deep-research-workshop/agentcore-runtime-role-arn\"\n",
    "    )[\"Parameter\"][\"Value\"],\n",
    "    entrypoint=\"agent.py\",\n",
    "    memory_mode=\"NO_MEMORY\",\n",
    "    requirements_file=\"requirements.txt\",\n",
    ")\n",
    "\n",
    "agentcore_runtime.launch(auto_update_on_conflict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ea9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from invoke_agentcore import invoke_agentcore\n",
    "import uuid\n",
    "\n",
    "session_id = str(uuid.uuid4())\n",
    "invoke_agentcore(\n",
    "    agent_runtime_name=\"pmc_gather_evidence_agent\",\n",
    "    prompt=\"How safe and effective are GLP-1 drugs for long term use?\",\n",
    "    session_id = session_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7541b4f1",
   "metadata": {},
   "source": [
    "## 5. (Optional) Interact with agent using a chat application\n",
    "\n",
    "To experiment with this agent in an instructor-led workshop, follow the steps in the Getting Started section of the Workshop Studio page for this event to access a Streamlit chat application. Then, select `pmc_gather_evidence_agent` from the **Agent Name** list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd841b58",
   "metadata": {},
   "source": [
    "## 6. (Optional) Clean Up\n",
    "\n",
    "Run the next notebook cell to delete the AgentCore runtime environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35fde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "agentcore_client = boto3.client(\"bedrock-agentcore-control\")\n",
    "agent_status = agentcore_runtime.status()\n",
    "\n",
    "agentcore_client.delete_agent_runtime(agentRuntimeId=agent_status.config.agent_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
