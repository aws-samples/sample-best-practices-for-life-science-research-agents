{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d534971-cb20-4752-9331-a3f98339eced",
   "metadata": {},
   "source": [
    "# Gather Evidence Using PaperQA2\n",
    "\n",
    "In this notebook, you'll use the PaperQA2 library from FutureHouse to gather evidence from PubMedCentral (PMC) articles.\n",
    "\n",
    "## 1. Prerequisites\n",
    "\n",
    "- Python 3.10 or later\n",
    "- AWS account configured with appropriate permissions\n",
    "- Access to the Anthropic Claude Sonnet 4 model on Amazon Bedrock\n",
    "- Basic understanding of Python programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722465ef-f70b-48fc-9b78-d4d8a49f30b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%pip install -U boto3 strands-agents strands-agents-tools defusedxml httpx bedrock_agentcore_starter_toolkit paper-qa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96912844-0b45-43e6-a233-286045130e7d",
   "metadata": {},
   "source": [
    "## 2. Experiment with PaperQA2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea721cf0",
   "metadata": {},
   "source": [
    "### 2.1. Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a5595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776787c7",
   "metadata": {},
   "source": [
    "Download example paper **LÃ¡la, Jakub, et al. \"PaperQA: Retrieval-Augmented Generative Agent for Scientific Research.\" arXiv, 2023, arxiv.org/abs/2312.07559**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ecd341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "# Create papers directory if it doesn't exist\n",
    "papers_dir = Path(\"my_papers/paperqa/text\")\n",
    "papers_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download the PDF\n",
    "url = \"https://arxiv.org/pdf/2312.07559\"\n",
    "output_path = papers_dir / \"paperqa.pdf\"\n",
    "\n",
    "print(f\"Downloading {url}...\")\n",
    "urllib.request.urlretrieve(url, output_path)\n",
    "print(f\"Downloaded to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0122dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperqa import Settings, ask\n",
    "from paperqa.settings import (\n",
    "    AgentSettings,\n",
    "    ParsingSettings,\n",
    "    IndexSettings,\n",
    "    AnswerSettings,\n",
    ")\n",
    "\n",
    "LLM = \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "\n",
    "answer_response = await ask(\n",
    "    \"What is PaperQA2?\",\n",
    "    settings=Settings(\n",
    "        llm=LLM,\n",
    "        summary_llm=LLM,\n",
    "        agent=AgentSettings(\n",
    "            agent_llm=LLM,\n",
    "            index=IndexSettings(\n",
    "                index_directory=\"my_papers/paperqa/index\",\n",
    "                paper_directory=\"my_papers/paperqa/text\",\n",
    "            ),\n",
    "        ),\n",
    "        embedding=\"bedrock/amazon.titan-embed-text-v2:0\",\n",
    "        parsing=ParsingSettings(use_doc_details=False),\n",
    "        answer=AnswerSettings(\n",
    "            answer_max_sources=1,\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e05c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_paperqa_results(answer) -> None:\n",
    "    \"\"\"Pretty print the output from the PaperQA2 query.\"\"\"\n",
    "    session_output = answer.session\n",
    "\n",
    "    print(\"**Question**\\n\")\n",
    "    print(session_output.question + \"\\n\")\n",
    "    print(f\"**Answer**\\n\")\n",
    "    print(session_output.answer + \"\\n\")\n",
    "    print(\"**Evidence**\\n\")\n",
    "    for context in session_output.contexts:\n",
    "        print(f\"{context.text.name}:\\t{context.context}\")\n",
    "    print(\"**Token Counts**\\n\")\n",
    "    print(f\"{'Model':<45} {'Input':<8} {'Output':<8}\")\n",
    "    print(\"-\" * 65)\n",
    "    for model, values in session_output.token_counts.items():\n",
    "        print(f\"{model:<45} {values[0]:<8} {values[1]:<8}\")\n",
    "    print()\n",
    "    print(\"**Estimated Cost**\\n\")\n",
    "    print(round(session_output.cost, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563ae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_paperqa_results(answer_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86a8f85",
   "metadata": {},
   "source": [
    "### 2.2. Cost Optimization with Multiple LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeac69f",
   "metadata": {},
   "source": [
    "Try different llms. From the docs:\n",
    "\n",
    "- `llm`: LLM for general use including metadata inference and answer generation. Should be 'best' LLM. Uses include:\n",
    "    1. Inferring citation information from documents when left unspecified\n",
    "    1. Extracting title, DOI, and authors from citation information when left unspecified\n",
    "    1. Optionally injecting pre-answer information\n",
    "    1. Generating an answer given evidence\n",
    "    1. Optionally injecting post-answer information \n",
    "- `summary_llm`: LLM for creating contextual summaries \n",
    "- `agent_llm`: LLM inside the agent making tool selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9cf32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperqa import Settings, ask\n",
    "from paperqa.settings import (\n",
    "    AgentSettings,\n",
    "    ParsingSettings,\n",
    "    IndexSettings,\n",
    "    AnswerSettings,\n",
    ")\n",
    "\n",
    "LLM = \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "SUMMARY_LLM = \"bedrock/openai.gpt-oss-120b-1:0\"\n",
    "AGENT_TYPE = \"fake\"\n",
    "EVIDENCE_K = 5\n",
    "EVIDENCE_SUMMARY_LENGTH = \"25 to 50 words\"\n",
    "\n",
    "answer_response = await ask(\n",
    "    \"What is PaperQA2?\",\n",
    "    settings=Settings(\n",
    "        llm=LLM,\n",
    "        summary_llm=SUMMARY_LLM,\n",
    "        agent=AgentSettings(\n",
    "            agent_llm=LLM,\n",
    "            index=IndexSettings(\n",
    "                index_directory=\"my_papers/paperqa/index\",\n",
    "                paper_directory=\"my_papers/paperqa/text\",\n",
    "            ),\n",
    "            agent_type=AGENT_TYPE,\n",
    "        ),\n",
    "        embedding=\"bedrock/amazon.titan-embed-text-v2:0\",\n",
    "        parsing=ParsingSettings(use_doc_details=False),\n",
    "        answer=AnswerSettings(\n",
    "            answer_max_sources=1,\n",
    "            evidence_k=EVIDENCE_K,\n",
    "            evidence_summary_length=\"about 50 words\",\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "pretty_print_paperqa_results(answer_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afcef46",
   "metadata": {},
   "source": [
    "By using a different model for the evidence summarization step, we were able to reduce the estimated cost by nearly half!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fdc510",
   "metadata": {},
   "source": [
    "### 2.3. Biomedical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1ad0b7",
   "metadata": {},
   "source": [
    "Use PMC11231910 as an example. First, download the text from RODA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a642a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import os\n",
    "\n",
    "s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "pmc_id = \"PMC9438179\"\n",
    "bucket_name = \"pmc-oa-opendata\"\n",
    "object_key = f\"oa_comm/txt/all/{pmc_id}.txt\"\n",
    "local_file_path = f\"my_papers/{pmc_id}.txt\"\n",
    "s3.download_file(bucket_name, object_key, local_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperqa import Settings, ask\n",
    "from paperqa.settings import (\n",
    "    AgentSettings,\n",
    "    ParsingSettings,\n",
    "    IndexSettings,\n",
    "    AnswerSettings,\n",
    ")\n",
    "\n",
    "LLM = \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "SUMMARY_LLM = \"bedrock/openai.gpt-oss-120b-1:0\"\n",
    "AGENT_TYPE = \"fake\"\n",
    "EVIDENCE_K = 5\n",
    "EVIDENCE_SUMMARY_LENGTH = \"25 to 50 words\"\n",
    "\n",
    "answer_response = await ask(\n",
    "    \"How safe and effective are GLP-1 drugs for long term use?\",\n",
    "    settings=Settings(\n",
    "        llm=LLM,\n",
    "        summary_llm=SUMMARY_LLM,\n",
    "        agent=AgentSettings(\n",
    "            agent_llm=LLM,\n",
    "            index=IndexSettings(\n",
    "                index_directory=\"my_papers/paperqa/index\",\n",
    "                paper_directory=\"my_papers/paperqa/text\",\n",
    "            ),\n",
    "            agent_type=AGENT_TYPE,\n",
    "        ),\n",
    "        embedding=\"bedrock/amazon.titan-embed-text-v2:0\",\n",
    "        parsing=ParsingSettings(use_doc_details=False),\n",
    "        answer=AnswerSettings(\n",
    "            answer_max_sources=1,\n",
    "            evidence_k=EVIDENCE_K,\n",
    "            evidence_summary_length=EVIDENCE_SUMMARY_LENGTH,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "pretty_print_paperqa_results(answer_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2831a-cad8-4c41-a0e5-9503e1a503f4",
   "metadata": {},
   "source": [
    "## 4. Create Strands Agent and tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d9b43",
   "metadata": {},
   "source": [
    "### 4.1. Call Gather Evidence Tool Directly\n",
    "\n",
    "There's an example of how to add PaperQA to a Strands tool in `gather_evidence.py`. Let's incorporate it into a test agent and call it directly. This may take 1-2 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from strands import Agent\n",
    "from gather_evidence import gather_evidence_tool\n",
    "\n",
    "MODEL_ID = \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "agent = Agent(tools=[gather_evidence_tool], model=MODEL_ID)\n",
    "\n",
    "agent.tool.gather_evidence_tool(\n",
    "    pmcid=\"PMC9438179\",\n",
    "    question=\"How safe and effective are GLP-1 drugs for long term use?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c0dea",
   "metadata": {},
   "source": [
    "### 4.2. Define Agent\n",
    "\n",
    "Let's define an agent that can use the `search_pmc` tool from notebook 01 and the new `gather_evidence` tool from this notebook to find and understand scientific articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fe2ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from search_pmc import search_pmc_tool\n",
    "from gather_evidence import gather_evidence_tool\n",
    "\n",
    "MODEL_ID = \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "QUERY = \"How safe and effective are GLP-1 drugs for long term use?\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a life science research assistant. When given a scientific question, follow this process:\n",
    "\n",
    "1. Use search_pmc_tool to find highly-cited papers. Search broadly first, then narrow down. Use temporal filters like \"last 2 years\"[dp] for recent work. \n",
    "2. Identify the PMC ID value for the 2 relevant papers, then submit their ID values and the query to the gather_evidence_tool.\n",
    "3. Generate a concise answer to the question based on the most relevant evidence, along with source citations\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Initialize your agent\n",
    "agent = Agent(\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    tools=[search_pmc_tool, gather_evidence_tool],\n",
    "    model=MODEL_ID,\n",
    ")\n",
    "\n",
    "# Send a message to the agent\n",
    "response = agent(QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b0d4f4",
   "metadata": {},
   "source": [
    "## 5. Deploy to Amazon Bedrock AgentCore Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130078be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedrock_agentcore_starter_toolkit import Runtime\n",
    "\n",
    "agentcore_runtime = Runtime()\n",
    "agentcore_runtime.configure(\n",
    "    agent_name=\"pmc_gather_evidence_agent\",\n",
    "    auto_create_ecr=True,\n",
    "    auto_create_execution_role=True,\n",
    "    entrypoint=\"agent.py\",\n",
    "    memory_mode=\"NO_MEMORY\",\n",
    "    requirements_file=\"requirements.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a982fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentcore_runtime.launch(auto_update_on_conflict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1707e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "agentcore_runtime.invoke(\n",
    "    {\"prompt\": \"How safe and effective are GLP-1 drugs for long term use?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7541b4f1",
   "metadata": {},
   "source": [
    "## 6. (Optional) Interact with agent using AgentCore Chat\n",
    "\n",
    "Follow these steps to open an interactive chat session with your new agent.\n",
    "\n",
    "1. Open a command line terminal in your notebook environment.\n",
    "2. Navigate to the project root folder (where `pyproject.toml` is located).\n",
    "3. Run `pip install .` to install the workshop tools including the chat CLI.\n",
    "4. Run `agentcore-chat` to launch the CLI.\n",
    "5. Select the `pmc_evidence_agent` by typing its name or index in the terminal and press Enter.\n",
    "6. Ask your question at the `You:` prompt and press Enter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd841b58",
   "metadata": {},
   "source": [
    "## 7. (Optional) Clean Up\n",
    "\n",
    "Run the next notebook cell to delete the AgentCore runtime environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35fde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "agentcore_client = boto3.client(\"bedrock-agentcore-control\")\n",
    "agent_status = agentcore_runtime.status()\n",
    "\n",
    "agentcore_client.delete_agent_runtime(agentRuntimeId=agent_status.config.agent_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
