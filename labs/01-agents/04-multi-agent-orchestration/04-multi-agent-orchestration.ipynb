{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d534971-cb20-4752-9331-a3f98339eced",
   "metadata": {},
   "source": [
    "# Multi-Agent Orchestration\n",
    "\n",
    "![Agent with scientific knowledge tool](../../../static/agent-multi.png)\n",
    "\n",
    "The final lab in this module demonstrates how to build a sophisticated multi-agent research system that addresses key challenges in multi-agent architectures, particularly the context window management and \"game of telephone\" problems identified in Anthropic's multi-agent research system blog.\n",
    "\n",
    "The lab implements a three-agent hierarchical system with clear separation of concerns:\n",
    "\n",
    "1. Lead Agent (Orchestrator)\n",
    "    - Role: Planning, coordination, and file management\n",
    "    - Responsibilities: Decomposes research questions, creates outlines, delegates tasks, and manages the overall workflow\n",
    "    - Tools: Research agent delegation, report generation tool, file editor\n",
    "    - Key Feature: Does NOT perform research activities directly, maintaining focus on orchestration\n",
    "2. Research Agent (Specialist)\n",
    "    - Role: Scientific literature investigation\n",
    "    - Responsibilities: Searches PMC databases, gathers evidence using PaperQA2's advanced RAG techniques\n",
    "    - Tools: PMC search tool, evidence gathering with DynamoDB storage\n",
    "    - Output: Concise research summaries with evidence IDs for citation tracking\n",
    "3. Writing Agent (Specialist)\n",
    "    - Role: Technical report generation\n",
    "    - Responsibilities: Creates properly cited reports using Anthropic's Claude Citations API\n",
    "    - Input: Research question and evidence IDs from DynamoDB\n",
    "    - Output: Professional reports with inline citations and reference lists\n",
    "\n",
    "Using multiple agents gives use additional LLM context to accomplish the research goal. It also allows us to precisely configure the system prompt, model, and tools for the task at hand.\n",
    "\n",
    "To address the \"game of telephone\" challenge, each research agent saves evidence records to a DynamoDb database and only passes the record IDs and a summary of the content back to the lead agent. The lead agent syntheiszes this information and passes the evidence ID values to the writing agent, which queries the DynamoDb table for the details necessary to write a high-quality, cited research report.\n",
    "\n",
    "## 1. Prerequisites\n",
    "\n",
    "- Python 3.12 or later\n",
    "- AWS account configured with appropriate permissions\n",
    "- Access to the Anthropic Claude 3.7 Sonnet model in Amazon Bedrock\n",
    "- Basic understanding of Python programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722465ef-f70b-48fc-9b78-d4d8a49f30b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -U boto3 strands-agents strands-agents-tools defusedxml httpx bedrock_agentcore_starter_toolkit paper-qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95097c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic\")\n",
    "warnings.filterwarnings(\"ignore\", module=\"litellm\")\n",
    "MODEL_ID = \"global.anthropic.claude-sonnet-4-20250514-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136e048e",
   "metadata": {},
   "source": [
    "## 2. Define PMC Research Agent\n",
    "\n",
    "This agent will be similar to the gather evidence agent we created in notebook 2, but with a change in how the evidence is stored. To avoid the \"game of telephone\" problem, we are going to deterministically store the gathered evidence in a DynamoDB table. This is a big advantage to building AI agents on AWS - they can leverage any of the 200+ services to store and process data.\n",
    "\n",
    "Let's test out the updated tool to see how it works.\n",
    "\n",
    "First, we create a DynamoDB table, with `toolUseId` as the primary key and `pmcid` as a global secondary id. This gives us the flexibility to extract evidence records either by tool use or paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b279954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "dynamodb = boto3.resource(\"dynamodb\")\n",
    "DDB_TABLE_NAME = \"deep-research-evidence\"\n",
    "os.environ[\"EVIDENCE_TABLE_NAME\"] = DDB_TABLE_NAME\n",
    "\n",
    "# Check if table exists\n",
    "try:\n",
    "    ddb_table = dynamodb.Table(DDB_TABLE_NAME)\n",
    "    ddb_table.load()\n",
    "    print(f\"Table '{DDB_TABLE_NAME}' already exists\")\n",
    "except:\n",
    "    # Create table if it doesn't exist\n",
    "    ddb_table = dynamodb.create_table(\n",
    "        TableName=DDB_TABLE_NAME,\n",
    "        KeySchema=[{\"AttributeName\": \"evidence_id\", \"KeyType\": \"HASH\"}],\n",
    "        AttributeDefinitions=[\n",
    "            {\"AttributeName\": \"evidence_id\", \"AttributeType\": \"S\"},\n",
    "            {\"AttributeName\": \"source\", \"AttributeType\": \"S\"},\n",
    "        ],\n",
    "        GlobalSecondaryIndexes=[\n",
    "            {\n",
    "                \"IndexName\": \"source_index\",\n",
    "                \"KeySchema\": [{\"AttributeName\": \"source\", \"KeyType\": \"HASH\"}],\n",
    "                \"Projection\": {\"ProjectionType\": \"ALL\"},\n",
    "            }\n",
    "        ],\n",
    "        BillingMode=\"PAY_PER_REQUEST\",\n",
    "    )\n",
    "\n",
    "    ddb_table.wait_until_exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ca42d2",
   "metadata": {},
   "source": [
    "Next, we directly invoke the updated `gather_evidence` tool and pass the db table name as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d665d50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from search_pmc import search_pmc_tool\n",
    "from gather_evidence_ddb import gather_evidence_tool\n",
    "\n",
    "agent = Agent(tools=[gather_evidence_tool], model=MODEL_ID)\n",
    "\n",
    "# Send a message to the agent\n",
    "agent.tool.gather_evidence_tool(\n",
    "    pmc_id=\"PMC9438179\",\n",
    "    question=\"How safe and effective are GLP-1 drugs for long term use?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c5870c",
   "metadata": {},
   "source": [
    "Let's look at the new records in our db table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be57396",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for record in ddb_table.scan().get(\"Items\"):\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Evidence ID: {record.get('evidence_id')}\")\n",
    "    print(f\"Question: {record.get('question')}\")\n",
    "    print(f\"Answer: {record.get('answer')}\")\n",
    "    print(f\"Source: {record.get('source')}\")\n",
    "    print(\"Context:\")\n",
    "    for i, item in enumerate(record.get('context'), start=1):\n",
    "        print(f\"  {i}: {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd36be0",
   "metadata": {},
   "source": [
    "We can also query for a records from a specific PMC ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ee6ccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from boto3.dynamodb.conditions import Key\n",
    "\n",
    "response = ddb_table.query(\n",
    "    IndexName=\"source_index\", KeyConditionExpression=Key(\"source\").eq('PMC9438179')\n",
    ")\n",
    "\n",
    "for record in ddb_table.scan().get(\"Items\"):\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Evidence ID: {record.get('evidence_id')}\")\n",
    "    print(f\"Question: {record.get('question')}\")\n",
    "    print(f\"Answer: {record.get('answer')}\")\n",
    "    print(f\"Source: {record.get('source')}\")\n",
    "    print(\"Context:\")\n",
    "    for i, item in enumerate(record.get('context'), start=1):\n",
    "        print(f\"  {i}: {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c7d9d",
   "metadata": {},
   "source": [
    "Let's incorporate our tool into a new `pmc_research_agent` definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba23226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from search_pmc import search_pmc_tool\n",
    "from gather_evidence_ddb import gather_evidence_tool\n",
    "\n",
    "QUERY = \"How safe and effective are GLP-1 drugs for long term use?\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a life science research assistant. When given a scientific question, follow this process:\n",
    "\n",
    "1. Use search_pmc_tool to find highly-cited papers. Search broadly first, then narrow down. Use temporal filters like \"last 2 years\"[dp] for recent work.\n",
    "2. Identify the PMC IDs of the most relevant papers, then submit each ID and the query to the gather_evidence_tool.\n",
    "3. Generate a concise answer to the question based on the most relevant evidence, followed by a list of the associated `evidence_id` values.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Initialize your agent\n",
    "pmc_research_agent = Agent(\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    tools=[search_pmc_tool, gather_evidence_tool],\n",
    "    model=MODEL_ID,\n",
    ")\n",
    "\n",
    "# Send a message to the agent\n",
    "response = pmc_research_agent(QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd264d1",
   "metadata": {},
   "source": [
    "Now let's view the updated records in our evidence table again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2261dd6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for record in ddb_table.scan().get(\"Items\"):\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Evidence ID: {record.get('evidence_id')}\")\n",
    "    print(f\"Question: {record.get('question')}\")\n",
    "    print(f\"Answer: {record.get('answer')}\")\n",
    "    print(f\"Source: {record.get('source')}\")\n",
    "    print(\"Context:\")\n",
    "    for i, item in enumerate(record.get('context'), start=1):\n",
    "        print(f\"  {i}: {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873da989",
   "metadata": {},
   "source": [
    "We'll use our `pmc_research_agent` again later in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6cf0ae",
   "metadata": {},
   "source": [
    "## 3. Define Technical Writing Agent\n",
    "\n",
    "Next, we'll build our writing agent. This agent will take in a question and evidence references and use the Anthropic Claude citations API to create a cited report. Let's test how this works with the Amazon Bedrock InvokeModel API \n",
    "\n",
    "Ref: https://docs.claude.com/en/docs/build-with-claude/citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31141f9f",
   "metadata": {},
   "source": [
    "### 3.1. Explore Anthropic Claude Citations API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393abd8f",
   "metadata": {},
   "source": [
    "Here is an example of how to use the Claude citations API through a Bedrock InvokeModel call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa47639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "example_request = {\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"document\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"media_type\": \"text/plain\",\n",
    "                        \"data\": \"Based on the available context, specific recommendations for managing gastrointestinal adverse events during GLP-1 receptor agonist (GLP-1 RA) therapy include several key strategies. Management emphasizes comprehensive patient education, initiating treatment with low starting doses, and implementing gradual dose titration to minimize adverse events (Gorgojo2023 chunk 2). Dietary adjustments and ongoing monitoring are also essential components of the management approach (Gorgojo2023 chunk 2).\\n\\nRegarding clinical experience with long-term safety, GI adverse events occur in 40-85% of patients receiving GLP-1 RAs, but these events are typically mild and transient in nature (Gorgojo2023 chunk 2). The adverse events generally resolve following dose escalation, indicating that patients can adapt to therapy over time (Gorgojo2023 chunk 2). \\n\\nLong-term safety data demonstrate a favorable profile, with most events classified as non-serious and low rates of permanent treatment discontinuation (Gorgojo2023 chunk 2). This clinical experience supports the overall safety profile of GLP-1 RAs for long-term use (Gorgojo2023 chunk 2). The combination of appropriate management strategies and the generally mild, self-limiting nature of GI adverse events allows most patients to continue therapy successfully over extended periods.\",\n",
    "                    },\n",
    "                    \"title\": \"PMC9821052\",\n",
    "                    \"citations\": {\"enabled\": True},\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"document\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"media_type\": \"text/plain\",\n",
    "                        \"data\": \"Long-term GLP-1 receptor agonist therapy demonstrates sustained efficacy in maintaining glycemic control and promoting weight loss (Zheng2024 chunk 55). However, therapeutic effectiveness may plateau over extended treatment periods, with studies indicating this limitation is associated with increased orbitofrontal reward activation, as observed with liraglutide (Zheng2024 chunk 55).\\n\\nThe adverse event profile is predominantly gastrointestinal, with nausea and vomiting representing the most common side effects (Zheng2024 chunk 55). Serious but rare adverse events include pancreatitis and gallbladder disease (Zheng2024 chunk 55).\\n\\nSeveral absolute contraindications exist for long-term GLP-1 receptor agonist use. Patients with a personal or family history of medullary thyroid carcinoma should not receive these agents (Zheng2024 chunk 55). Multiple endocrine neoplasia type 2 (MEN 2) also represents a contraindication (Zheng2024 chunk 55). Additionally, severe gastrointestinal disorders preclude the use of these medications (Zheng2024 chunk 55).\\n\\nThe safety and efficacy profile supports long-term use in appropriate patients, though careful patient selection is essential given the specific contraindications, particularly those related to thyroid malignancy risk and pre-existing gastrointestinal pathology.\",\n",
    "                    },\n",
    "                    \"title\": \"PMC9821052\",\n",
    "                    \"citations\": {\"enabled\": True},\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is the long-term safety profile and effectiveness of GLP-1 receptor agonists? What are the main adverse events and contraindications for long-term use?\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "}\n",
    "\n",
    "response = bedrock_client.invoke_model(\n",
    "    modelId=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "    contentType=\"application/json\",\n",
    "    accept=\"application/json\",\n",
    "    body=json.dumps(example_request),\n",
    ")\n",
    "\n",
    "response_body = json.loads(response[\"body\"].read())\n",
    "\n",
    "for content in response_body.get(\"content\"):\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73abb879",
   "metadata": {},
   "source": [
    "We can now format the response into a citated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef9effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cited_response(response_content: dict) -> None:\n",
    "    citations = []\n",
    "    for content_item in response_content:\n",
    "        print(content_item.get(\"text\"), end=\"\")\n",
    "        for citation in content_item.get(\"citations\", []):\n",
    "            title = citation.get(\"document_title\")\n",
    "            if title not in citations:\n",
    "                citations.append(title)\n",
    "            print(f\" ({citations.index(title)+1})\", end=\"\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"## References\")\n",
    "    for i, title in enumerate(citations, start=1):\n",
    "        print(f\"{i}. https://www.ncbi.nlm.nih.gov/pmc/articles/{title}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "print_cited_response(response_body.get(\"content\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d4b1e",
   "metadata": {},
   "source": [
    "### 3.2. Create generate_report tool\n",
    "\n",
    "We've provided a Strands tool definition that incorporates this code in `generate_report.py'. Let's test it with the example records from earlier in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a533c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [record for record in ddb_table.scan().get(\"Items\")[:2]]\n",
    "question = records[0][\"question\"]\n",
    "evidence = [record.get(\"evidence_id\") for record in records]\n",
    "\n",
    "print(question)\n",
    "print(evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f712863b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from generate_report import generate_report_tool\n",
    "\n",
    "MODEL_ID = \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "agent = Agent(tools=[generate_report_tool], model=MODEL_ID)\n",
    "\n",
    "result = agent.tool.generate_report_tool(\n",
    "    prompt=f\"Please write a concise report that answers the question, {question}\",\n",
    "    evidence_ids=evidence,\n",
    ")\n",
    "\n",
    "print(result.get(\"content\")[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72739847",
   "metadata": {},
   "source": [
    "This \"agent\" can be used by our lead agent to generate cited, well-written final reports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b4881e",
   "metadata": {},
   "source": [
    "## 4. Create Lead Agent\n",
    "\n",
    "Now that we've defined all of our subagents and tools, we're ready to create our lead agent. This agent will be responsible for planning, updating files, and assigning tasks but NOT to any of the research activities. This separation of concerns allows us to opimize the token usage and simplify the agent definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d3f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmc_research_agent import pmc_research_agent\n",
    "from strands import Agent, tool\n",
    "\n",
    "@tool\n",
    "def research_agent(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    AI agent for researching scientific questions using articles from PubMed Central (PMC).\n",
    "\n",
    "    You may delegate research tasks to this agent by providing clear text instructions in the prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt: Scientific question to research using articles from PMC\n",
    "\n",
    "    Returns:\n",
    "        Concise answer to the question based on the most relevant evidence, followed by a list of the associated `evidence_id` values for citation analysis.\n",
    "    \"\"\"\n",
    "    return pmc_research_agent(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dba2184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from strands.models import BedrockModel\n",
    "from strands_tools import editor\n",
    "from pmc_research_agent import pmc_research_agent\n",
    "from generate_report import generate_report_tool\n",
    "from lead_config import SYSTEM_PROMPT, MODEL_ID\n",
    "\n",
    "import os\n",
    "\n",
    "model = BedrockModel(\n",
    "    model_id=MODEL_ID,\n",
    "    max_tokens=10000,\n",
    "    cache_prompt=\"default\",\n",
    "    temperature=1,\n",
    "    additional_request_fields={\n",
    "        \"anthropic_beta\": [\"interleaved-thinking-2025-05-14\"],\n",
    "        \"reasoning_config\": {\n",
    "            \"type\": \"enabled\",\n",
    "            \"budget_tokens\": 3000,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "os.environ[\"BYPASS_TOOL_CONSENT\"] = \"true\"\n",
    "\n",
    "lead_agent = Agent(\n",
    "    model=model,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    tools=[research_agent, generate_report_tool, editor],\n",
    ")\n",
    "\n",
    "response = lead_agent(\n",
    "    \"How safe and effective are GLP-1 drugs for long term use? Please limit your output report to only 3 sections\"\n",
    ")\n",
    "response = lead_agent(\"I approve the outline. Please proceed.\")\n",
    "\n",
    "response.metrics.accumulated_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08dfa7f",
   "metadata": {},
   "source": [
    "## 5. Deploy to Amazon Bedrock AgentCore Runtime\n",
    "\n",
    "Let's look at the new agent definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat lead_agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6cfe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from bedrock_agentcore_starter_toolkit import Runtime\n",
    "\n",
    "ssm = boto3.client('ssm')\n",
    "\n",
    "agentcore_runtime = Runtime()\n",
    "agentcore_runtime.configure(\n",
    "    agent_name=\"lead_agent\",\n",
    "    auto_create_ecr=True,\n",
    "    execution_role=ssm.get_parameter(Name='/deep-research-workshop/agentcore-runtime-role-arn')['Parameter']['Value'],\n",
    "    entrypoint=\"lead_agent.py\",\n",
    "    memory_mode=\"NO_MEMORY\",\n",
    "    requirements_file=\"requirements.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d438cac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentcore_runtime.launch(auto_update_on_conflict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "session_id = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744c40d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from invoke_agentcore import invoke_agentcore\n",
    "invoke_agentcore(\n",
    "    agent_runtime_name=\"lead_agent\",\n",
    "    prompt=\"How safe and effective are GLP-1 drugs for long term use?\",\n",
    "    session_id = session_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc97d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from invoke_agentcore import invoke_agentcore\n",
    "invoke_agentcore(\n",
    "    agent_runtime_name=\"pmc_deep_research_agent\",\n",
    "    prompt=\"Looks good! Please continue with section 1.\",\n",
    "    session_id = session_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a0a4b",
   "metadata": {},
   "source": [
    "## 6. (Optional) Interact with agent using AgentCore Chat\n",
    "\n",
    "Follow these steps to open an interactive chat session with your new agent.\n",
    "\n",
    "1. Open a command line terminal in your notebook environment.\n",
    "2. Navigate to the project root folder (where `pyproject.toml` is located).\n",
    "3. Run `pip install .` to install the workshop tools including the chat CLI.\n",
    "4. Run `agentcore-chat` to launch the CLI.\n",
    "5. Select the `lead_agent` by typing its name or index in the terminal and press Enter.\n",
    "6. Ask your question at the `You:` prompt and press Enter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e0562a",
   "metadata": {},
   "source": [
    "## 7. (Optional) Clean Up\n",
    "\n",
    "Run the next notebook cell to delete the AgentCore runtime environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "agentcore_client = boto3.client(\"bedrock-agentcore-control\")\n",
    "agent_status = agentcore_runtime.status()\n",
    "\n",
    "agentcore_client.delete_agent_runtime(agentRuntimeId=agent_status.config.agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98238c41-d41c-4f85-b1f1-e2a3d730d0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
